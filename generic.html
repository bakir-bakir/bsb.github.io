<!DOCTYPE html>
<html>
  <head>
    <title>Portfolio Website</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Header -->
      <header id="header">
        <a href="index.html" class="logo">A Full Description of the Projects</a>
      </header>

      <!-- Nav -->
      <nav id="nav">
        <ul class="links">
          <li><a href="index.html">Projects</a></li>
          <li class="active"><a href="generic.html">Descriptions</a></li>
        </ul>
        <ul class="icons">
          <li>
            <a href="#" class="icon brands fa-github"
              ><span class="label">GitHub</span></a
            >
          </li>
        </ul>
      </nav>

      <!-- Main -->
      <div id="main">
        <!-- Post -->
        <section class="post">
          <h2>COVID-19 Project</h2>
          <p>
            Source: ourworldindata.org<br />
            Tools Used: MS SQL Server Management System, MS PowerBI, Tablaeau<br />
            <br />
            The aim of this exploratory data analysis (EDA) project is to
            analyze and gain insights into the global COVID-19 pandemic using
            data related to cases, deaths, and vaccinations. The project
            utilizes SQL Server as the database management system to extract,
            transform, and analyze the data.<br />
            <br />
            The dataset used in this project consists of multiple tables
            containing information on COVID-19 cases, deaths, and vaccinations
            from various countries around the world. The tables are structured
            in a relational database schema, allowing for efficient querying and
            analysis.<br />
            <br />
            Data Preparation and Cleaning:<br />

            Importing the dataset into SQL Server and ensuring data integrity.
            Handling missing or inconsistent values, and removing any outliers
            if necessary.<br />
            <br />
            Exploratory Data Analysis:<br />

            1.Conducting an initial overview of the dataset to understand its
            structure and variables.<br />
            2.Analyzing the distribution of COVID-19 cases, deaths, and
            vaccinations across different regions and countries.<br />
            3.Examining temporal trends, including daily, weekly, or monthly
            variations in cases, deaths, and vaccinations.<br />
            4.Investigating correlations between variables to identify potential
            relationships or dependencies.<br />
            <br />
            Data Visualization:<br />

            Creating visually appealing charts, graphs, and maps to present the
            findings effectively. Visualizing the geographic distribution of
            cases, deaths, and vaccination rates using PowerBI and Tableau. And
            generating interactive visualizations to allow users to explore the
            data and make meaningful observations.<br />
            <br />
            Insights and Interpretation:<br />

            Deriving meaningful insights from the analysis, such as identifying
            countries with high infection rates, understanding the impact of
            vaccination campaigns, or determining the correlation between case
            rates and mortality rates.<br />

            Throughout the project, SQL Server's powerful querying capabilities
            are utilized to extract, transform, and analyze the data
            efficiently. The goal is to uncover meaningful patterns,
            relationships, and trends that can contribute to our understanding
            of the global COVID-19 pandemic.<br />

            By conducting this exploratory data analysis, we aim to provide
            valuable insights that can support policymakers, researchers, and
            healthcare professionals in making informed decisions and taking
            appropriate actions to combat the ongoing pandemic.
          </p>
          <h2>Nashville Housing Project</h2>
          <p>
            Source: kaggle.com <br />Tools Used: MS SQL Server Management
            System<br />
            <br />The dataset used for this project comprises housing-related
            information from various sources, including property listings, sales
            records, and demographic data specific to Nashville, Tennessee. It
            includes attributes such as property addresses, sale prices, square
            footage, number of bedrooms and bathrooms, and other relevant
            features.<br />
            <br />
            The project involves several key steps:<br />

            Data Import: Import the raw dataset into SQL Server, ensuring that
            the data is correctly formatted and loaded into appropriate
            tables.<br />

            Data Exploration: Conduct an initial exploration of the dataset to
            gain a deeper understanding of its structure, content, and potential
            data quality issues. Identify potential inconsistencies, missing
            values, outliers, or anomalies that need to be addressed.<br />
            <br />
            Data Cleaning and Transformation: Apply a range of data cleaning
            techniques to improve the dataset's quality and consistency, such
            as:<br />

            1.Removing duplicates: Identify and eliminate duplicate records from
            the dataset to avoid redundancies and ensure data accuracy.<br />

            2.Handling missing values: Assess the extent of missing values
            across different attributes and employ strategies like imputation or
            deletion, based on the nature and impact of the missingness.<br />

            3.Standardizing formats: Standardize and format data fields like
            addresses, phone numbers, and dates to ensure consistency and
            facilitate further analysis.<br />

            4.Correcting inconsistencies: Identify and rectify inconsistencies
            in attribute values, such as erroneous entries, misspellings, or
            conflicting information.<br />

            5.Validating data integrity: Implement data integrity checks and
            constraints to ensure that the data adheres to defined rules and
            constraints.
          </p>
          <h2>IMDB Movies Project</h2>
          <p>
            Source: kaggle.com <br />Tools Used: Python (Pandas, Seaborn,
            Matplotlib, NumPy)<br />
            <br />The dataset used for this project comprises housing-related In
            this data analysis project, we explore an IMDB Movies dataset using
            Python to gain insights into the relationships between budget, gross
            revenue, and production companies. The project involves data
            cleaning, visualization, and the study of a correlation matrix to
            uncover patterns and trends within the movie industry.<br />

            The dataset used for this analysis contains information about
            various movies, including attributes such as budget, gross revenue,
            production companies, and other relevant features. The dataset
            provides a comprehensive view of the movie landscape, enabling us to
            delve into the financial aspects and production dynamics of the
            industry.<br />

            The project involves the following key steps:<br />
            <br />
            1.Data Cleaning: Employing Python's data manipulation libraries, we
            clean the dataset to ensure data integrity and consistency. This
            includes handling missing values, removing duplicates, and
            addressing any inconsistencies in the data.<br />
            <br />
            2.Exploratory Data Analysis: Conducting an initial exploration of
            the dataset to understand its structure and contents. We examine the
            distribution of budget and gross revenue, identify outliers, and
            explore the composition of production companies within the
            dataset.<br />
            <br />
            3.Data Visualization: Creating visualizations using Python's data
            visualization libraries (e.g., Matplotlib, Seaborn) to gain insights
            into the relationships between budget, gross revenue, and production
            companies. We generate various types of charts, such as scatter
            plots, bar charts, and box plots, to visually analyze the data and
            identify any significant patterns or trends.<br />
            <br />
            4.Correlation Analysis: Utilizing Python's statistical libraries
            (e.g., NumPy, Pandas), we calculate and study the correlation matrix
            to understand the relationships between budget, gross revenue, and
            production companies. This analysis helps determine if there are any
            statistically significant correlations or dependencies among these
            variables.<br />
            <br />
            5.Insights and Interpretation: Deriving meaningful insights from the
            analysis, we explore questions such as the impact of budget on gross
            revenue, the distribution of revenue across different production
            companies, and the relationship between budget and the success of
            movies. We also investigate any notable trends or outliers that may
            influence these relationships.<br />
            <br />
            Throughout the project, Python's powerful data analysis and
            visualization capabilities are leveraged to extract insights from
            the IMDB Movies dataset. By studying the interplay between budget,
            gross revenue, and production companies, we aim to provide valuable
            insights for movie industry professionals, investors, and
            stakeholders.<br />
          </p>
          <h2>YouTube API Web Scrapper</h2>
          <p>
            Source: youtube.com <br />Tools Used: Python (Pandas, Seaborn,
            Matplotlib)<br />
            <br />The dataset used for this project comprises housing-related
            This Python program is designed to connect to the YouTube API using
            a Google Cloud API key, enabling the retrieval of data from multiple
            YouTube channels. The program extracts video data from the specified
            channels using their unique channel keys. The retrieved data is then
            cleaned, appropriately grouped, and visualized to gain insights into
            the video trends of a particular channel.<br />
            <br />
            The program involves the following key steps:<br />

            YouTube API Connection Setup: Using the Google Cloud API key, we
            establish a connection to the YouTube API. This allows us to
            authenticate and interact with the YouTube platform
            programmatically.<br />

            Channel Data Retrieval: Using the YouTube API, we fetch data from
            several YouTube channels by providing their respective channel keys.
            This data may include attributes such as channel name, subscriber
            count, view count, and other relevant statistics.<br />
            <br />
            Video Data Extraction: Focusing on a specific channel of interest,
            we extract video-related data from that channel. This includes
            details such as video titles, upload dates, view counts, like
            counts, and comment counts.<br />
            <br />
            Data Cleaning: To ensure data accuracy and consistency, we perform
            cleaning operations on the extracted video data. This involves
            handling missing values, removing duplicates, and addressing any
            inconsistencies in the dataset.<br />
            <br />
            Visualization Creation: Utilizing Python's data visualization
            libraries, such as Matplotlib or Seaborn, we generate visual
            representations of the video data. This may include bar charts, line
            graphs, or pie charts, providing a visual overview of video trends,
            viewership, or engagement metrics.<br />
            <br />
            The program's ability to connect to the YouTube API and retrieve
            data from multiple channels enhances the understanding of the video
            landscape. By cleaning and grouping the video data, we gain valuable
            insights into a specific channel's performance, audience engagement,
            and content trends. The visualizations enable effective
            communication of these insights and aid in decision-making
            processes.
          </p>
	  <h2>Netflix Project - EDA Python</h2>
	  <p>
Description:
In this data analysis project, we dive into a comprehensive dataset that contains information about Netflix movies, TV shows, genres, directors, ratings, and more.
Using Python's powerful data analysis libraries such as Pandas, NumPy, Matplotlib, and Seaborn, we explore, format, analyze, and visualize the data to gain
meaningful insights into the world of Netflix content.<br />
<br />
The project encompasses the following key steps:<br />
<br />
Data Formatting and Cleaning: Initially, we load the dataset into a Pandas DataFrame and perform necessary data formatting and cleaning operations. This involves
handling missing values, removing duplicates, and ensuring consistency in data types and formats. By transforming the dataset into a clean and structured format,
we lay the groundwork for subsequent analysis.<br />
<br />
Exploratory Data Analysis: With the formatted dataset, we conduct exploratory data analysis (EDA) to gain a better understanding of the Netflix content landscape.
We analyze various aspects such as the distribution of movies and TV shows, genres, release years, directors, and ratings. This EDA phase helps identify trends,
patterns, and interesting insights within the dataset.<br />
<br />
Data Analysis and Insights: Leveraging the capabilities of Pandas, NumPy, and other Python libraries, we perform deeper analysis to uncover key insights. We
explore relationships between variables, such as the correlation between ratings and genres or the distribution of directors across different genres. We also
analyze the distribution of content by release year and explore any notable trends or patterns.<br />
<br />
Data Visualization: To effectively communicate the findings, we create visualizations using Matplotlib and Seaborn. These visual representations include bar plots,
line charts, histograms, and heatmaps, among others. Through compelling visuals, we highlight the distribution of content, showcase trends, and convey key insights
discovered during the analysis phase.<br />
		  <br />
	  </p>
          <h2>CMC API Web Scrapper</h2>
          <p>
            Source: youtube.com <br />Tools Used: Python (Pandas, Seaborn,
            Matplotlib)<br />
            <br />
            In this Python program, we leverage the CoinMarketCap API to
            establish a connection with the coinmarketcap.com website and
            retrieve real-time data on cryptocurrency market trends. The program
            includes features such as continuous data retrieval through a timer
            function, data cleaning to ensure accuracy, and the creation of
            visualizations for effective data representation.<br />

            The program encompasses the following main steps:<br />
            <br />
            API Connection Setup: Using Python's requests library, we establish
            a connection to the CoinMarketCap API. This allows us to send HTTP
            requests and retrieve data in JSON format.<br />
            <br />
            Continuous Data Retrieval: Implementing a timer function, we
            schedule periodic data retrieval from the CoinMarketCap API. This
            ensures that the program consistently pulls the latest information
            on cryptocurrency market trends at predefined intervals.<br />
            <br />
            Data Cleaning: Upon receiving the data from the API, we perform data
            cleaning processes to handle any inconsistencies, missing values, or
            outliers. This ensures the data is accurate and reliable for further
            analysis and visualization.<br />
            <br />
            Data Grouping and Aggregation: Depending on the specific analysis
            goals, we group and aggregate the retrieved data based on relevant
            factors such as cryptocurrency type, market capitalization, price,
            or trading volume. This enables us to derive meaningful insights
            from the dataset.<br />
            <br />
            Visualization Creation: Utilizing Python's data visualization
            libraries, such as Matplotlib or Plotly, we create visually
            appealing and informative charts, graphs, or interactive
            visualizations. These visual representations help in understanding
            the trends, patterns, and relationships within the cryptocurrency
            market
          </p>
        </section>
      </div>

      <!-- Footer -->
      <footer id="footer"></footer>

      <!-- Copyright -->
      <div id="copyright">
        <ul>
          <li>&copy; Untitled</li>
          <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
        </ul>
      </div>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
